from eval.metrics.dependencies import qa_eval_engine
from eval.metrics.models import EntityExtraction


async def get_entities(
    *, user_prompt: str, llm_answer: str, expected_answer: str
) -> EntityExtraction:
    """
    Extract entities from the LLM answer using Azure OpenAI.

    This function prepares the necessary messages and invokes the model to
    extract entities based on the provided user prompt, LLM answer, and
    expected answer.

    Args:
        user_prompt (str): The original user query or prompt.
        llm_answer (str): The answer generated by the language model.
        expected_answer (str): The ground truth answer for comparison.

    Returns:
        EntityExtraction: The extracted entities from the LLM answer.
    """

    return await qa_eval_engine().entity_extraction(
        user_prompt, llm_answer, expected_answer
    )
